分布式系统，尤其是分布式存储系统，需要解决的两个最主要的问题:
- 数据分片
- 数据冗余

下图形象生动地解释了其概念和区别：
![image](http://p99.pstatp.com/large/pgc-image/1537234805118feb9a5b2be)

其中数据A、B即属于数据分片，原始数据被拆分成两个正交子集分布在两个节点上。而数据集C属于数据冗余，同一份完整的数据在两个节点都有存储。当然，在实际的分布式系统中，数据分片和数据冗余一般都是共存的。

> 所谓分布式系统，就是利用多个独立的计算机来解决单个节点（计算机）无法处理的存储、计算问题。每个节点只负责原问题（即整个系统需要完成的任务）的一个子集。

可是原问题如何拆分到多个节点？在分布式存储系统中，任务的拆分即**数据分片**。

- 数据分片（segment，fragment，shard，partition），就是按照一定的规则，将数据集划分成相互独立、正交的数据子集，然后将数据子集分布到不同的节点上。
- 注意：数据分片需要按照一定的规则，++不同的分布式应用有不同的规则++，但都遵循同样的原则：**按照最主要、最频繁使用的访问方式来分片**。

# 数据分片方式
- Hash方式
- 一致性Hash
- 按照数据范围

对于任何方式，都需要思考以下几个问题：
1. 具体如何划分原始数据集？（**数据的切分**）
2. 当原问题的规模变大时，能否通过增加节点来动态适应？（**动态扩容**）
3. 当某个节点故障时，能否将该节点上的任务均衡的分摊到其他节点？（**故障转移**）
4. 对于可修改的数据（如数据库数据），如果某节点数据量变大，能否以及如何将部分数据迁移到其他负载较小的节点，达到动态均衡的效果？（**负载均衡**）
5. 元数据管理规模，元数据更新的频率以及复杂度？（**元数据管理**）


## Hash方式
哈希表（散列表）是最为常见的数据结构，根据记录（或者对象）的关键值将记录映射到表中的一个槽（slot），便于快速访问。

绝大多数编程语言都有对hash表的支持，如:

编程语言 | 实现方式
---|---
Python|dict
C++|map
Java|Hashtable
Lua|table

在哈希表中，最为简单的**散列函数**是mod N（N为表的大小），即首先将关键值计算出hash值（一个整型），通过对N取余，余数即在表中的位置。

数据分片的hash方式也是这个思想，即按照数据的某一**特征**（key）来计算哈希值，并将哈希值与系统中的节点建立映射关系，从而将哈希值不同的数据分布到不同的节点上。

### 优点
映射关系简单，需要管理的元数据非常少，只需要记录节点的数目以及Hash方式。

### 缺点
1. 当加入或者删除一个节点时，大量的数据需要移动。

> 这种方式的不满足**单调性**的：如果已经有一些内容通过Hash分派到了相应缓冲，又有新的缓冲加入到系统中，Hash的结果应该能够保证原有分派的内容可以被映射到原有的或者新的缓冲中，而不会被映射到旧的缓冲集合中的其他缓冲区。

2. 难以解决数据不均衡的问题。具体情况如下：
    -   原始数据的特征值分布不均匀，导致大量的数据集中到一个物理节点上。
    -   对于可修改的记录数据，单条记录的数据变大。
这两种情况都会导致节点之间的负载不均衡。

## 一致性Hash
一致性Hash是将数据按照特征值映射到一个首尾相接的Hash环上，同时也将节点（按照IP地址或者主机名）映射到这个环上。

对于数据，从数据在环上的位置开始，顺时针找到的第一个节点即为数据的存储节点。

### 举个例子
1. 数据范围[0-1000]
2. 节点的Hash值：N1(100),N2(400),N3(800)

Hash环示意图与数据的分布如下：

![image](http://p9.pstatp.com/large/pgc-image/1537234804988354ff27c43)

![image](http://p3.pstatp.com/large/pgc-image/15372348051406b320ce5ce)

### 优点
- 一致性Hash需要维护的元数据额外包含节点在环上的位置（这个数据量非常小）。
- 一致性Hash在增删节点的时候，受到影响的数据比较有限，只会影响到Hash环上相应的节点，不会出现大规模的数据迁移。

### 缺点
一致性Hash在增加节点的时候，只能分摊一个节点的压力，或者一个节点挂掉的时候，该节点的压力会全部转移到下一个节点。

> 在实际应用中，引入虚拟节点，而不是将物理节点映射到Hash环上。虚拟节点的数目远大于物理节点，因此一个物理节点需要负责多个虚拟节点的真实存储。**操作数据的时候，先通过Hash环找到对应的虚拟节点，然后通过虚拟节点找到对应的物理节点。**

引入虚拟节点后，一致性Hash需要维护的元数据也会增多：
1. 虚拟节点在Hash环上的问题，且虚拟节点的数目又比较多
2. 虚拟节点与物理节点的映射关系

当一个物理节点失效时，对应的Hash环上有多个虚拟节点失效，对应的压力也会分散到多个其他的虚拟节点上（即多个其他的物理节点上）。增加物理节点也是同样的效果。

在工程中，Cassandra使用一致性hash算法，并加入虚拟节点概念。

**引入数据副本后，一致性Hash方式需要做相应调整。**

# 按照数据范围（range based）
按照关键值划分成不同的区间，每个物理节点上负责一个或者多个区间。

> 与一致性Hash有点像，可以理解为物理节点在Hash环上是动态变化的。

**注意：每个节点的数据区间大小是不固定的，每个数据区间的数据量和区间大小也没有关系**。比如，一部分数据非常集中，那么区间大小应该是比较小的，即以数据量的大小为片段标准。

在实际应用中，一个节点往往负责多个区间，每个区间成为一个块（chunk、block），每个块有一个阈值，当达到这个阈值之后，就会分裂成为两块。**这样的目的是在有新的节点加入的时候，可以快速达到均衡的目的。**

- 如果一个节点负责的数据只有一个区间，range based与没有虚拟节点的一致性Hash很类似。
- 如果一个节点负责多个区间，range based 与有虚拟节点的一致性Hash很类型。


range based的元数据管理相对复杂，需要记录每个节点的数据区间范围，特别是单个节点多个分区的情况。而且，在数据可修改的情况下，如果块进行分裂，那么元数据中的区间信息也需要同步修改。

**广泛应用于MangoDB、PostgreSQL、HDFS等**。

## 总结

分片方式|映射难度|元数据|节点增删|数据动态均衡
---|---|---|---|---|---|---|
Hash|简单|非常简单，几乎不用修改|需要迁移的数据较多|不支持
一致性Hash|简单|比较简单，取决于节点规律，几乎不用修改|只影响Hash环上相邻的节点，不能使所有节点都参与数据迁移|不支持
一致性Hash（有虚拟节点）|中等|稍微复杂一些，取决于虚拟节点规律，很少修改|需要迁移的数据较少，所有节点都能贡献部分数据|修改虚拟节点与物理节点的映射可以实现动态均衡
range based|较为复杂|取决于每个块的大小，一般来说规律较大，且修改频率较高|需要迁移的数据比较少，所有节点都贡献部分数据|支持，容易实现


# 特征值选择
特征值的选择，基于最常访问的模式。访问时包括对数据的增删改查。

### 举个例子
选择“ID”作为分片的依据，就是默认数据的增删改查都是通过“ID”字段进行的。

如果在应用中，大量的数据操作都是通过这个特征值进行，那么数据分片就能带来以下好处：
1. 提升性能和并发，操作被分发到不同分片，相互独立
2. 提高系统的可用性，部分分片不可用，其他分片不受影响

> 如果大量的操作并没有使用到特征值，而元数据中记录的是按照特征值映射的数据位置，此时，查找其他值就需要在多个分片上查找后再聚合。

如果多条数据拥有相同的ID字段，那么这些数据肯定会分布到同一个节点上，这会带来两个问题：
1. 不能起到节点间数据的负载均衡
2. 如果数据超过单节点承载能力，如何处理？增加节点也无济于事。



# 元数据服务器
记录数据与节点的映射关系、节点状态等信息的服务器，称为元数据服务器。

元数据服务器的高性能，高可用（不能单点故障）和高可扩展，来应对元数据的增长。

## 高可用
实现方式：
1. 主从同步
2. 分布式一致性协议

## 元数据缓存
即使元数据服务器可以由一组物理机器组成，也保证了副本集之间的一致性问题。但是如果每次对数据的请求都经过元数据服务器的话，元数据服务器的压力也是非常大的。很多应用场景，元数据的变化并不是很频繁，因此可以在访问节点上做缓存，这样应用可以直接利用缓存数据进行数据读写，减轻元数据服务器压力。

在这个环境下，缓存的元数据必须与元数据服务器上的元数据一致，缓存的元数据必须是准确的、未过时的。相反的例子是DNS之类的缓存，即使使用了过期的DNS缓存也不会有太大的问题。

怎么达到缓存的强一致性呢？比较容易想到的办法是当metadata变化的时候立即通知所有的缓存服务器（mongos），但问题是通信有延时，不可靠。

解决不一致问题：
- 一个常见的思路是版本号。
- 另一种方式是Lease机制。

### Lease机制
1. 服务器向所有客户端发送缓存数据的同时，颁发一个lease，lease包含一个有限期（即过期时间）；

> lease的含义是：在这个有效期内，服务器保证元数据不会发生变化；

2. 客户端在这个有效期内可以放心大胆的使用缓存的元数据，如果超过了有效期，就不能使用数据了，就得去服务器请求；
3. 如果外部请求修改服务器上的元数据（元数据的修改一定在服务器上进行），那么服务器会阻塞修改请求，直到所有已颁发的lease过期，然后修改元数据，并将新的元数据和新的lease发送到客户端；
4. 如果元数据没有发生变化，那么服务器也需要在之前已颁发的lease到期之间，重新给客户端颁发新的lease（只有lease，没有数据）。

lease 的容错关键在于，只要服务器一旦发出数据和lease，不关心客户端是否收到数据，只要等待lease过期，就可以修改元数据；另外，lease的有效期通过过期时间（一个时间戳）来标识，因此即使从服务器到客户端的消息延时到达、或者重复发送都是没有关系的。

**容错的前提是服务器与客户端的时间要一致。**

1. 如果服务器的时间比客户端的时间慢，那么客户端收到lease之后很快就过期了，lease机制就发挥不了作用；
2. 如果服务器的时间比客户端的时间快，那么就比较危险，因为客户端会在服务器已经开始更新元数据的时候继续使用缓存，工程中，通常将服务器的过期时间设置得比客户端的略大，来解决这个问题；
3. 为了保持时间的一致，最好的办法是使用NTP（Network Time Protocol）来保证时钟同步。

-----
lease机制的本质是颁发者授予的在某一有效期内的承诺，承诺的范围是非常广泛的：

1. 比如上面提到的cache；
2. 比如做权限控制，例如当需要做并发控制时，同一时刻只给某一个节点颁发lease，只有持有lease的节点才可以修改数据；
3. 比如身份验证，例如在primary-secondary架构中，给节点颁发lease，只有持有lease的节点才具有primary身份；
4. 比如节点的状态监测，例如在primary-secondary架构中监测primary是否正常。

