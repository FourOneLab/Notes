[Istio](https://istio.io/)

[中文文档](https://preliminary.istio.io/zh/)

**Istio的源头是微服务。**

> An open platform to connect, secure, control and observe services.

连接、保护、控制和观测服务(**微服务**)的开放平台（**平台开源**）。

- 连接（Connect）：智能控制服务之间的流量和API调用，进行一些列测试，并通过红/黑部署逐渐升级。
- 保护（Secure）：通过托管身份验证、授权和服务之间通信加密自动保护服务。
- 控制（Control）：应用策略并确保其执行使得资源在消费者之间公平分配。
- 观测（Observe）：通过丰富的自动跟踪、监控和记录所以服务，了解正在发生的情况。

# Service Mesh
**借着微服务和容器化的东风，传统的代理摇身一变，成了如今炙手可热的 Service Mesh。**

有网络访问的地方就会有代理的存在。

代理可以是嵌套的，也就是说通信双方 A、B 中间可以多层代理，而这些代理的存在有可能对 A、B 是透明的。

![image](http://p99.pstatp.com/large/pgc-image/15401920262142c9b170d4d)

代理可以为整个通信带来更多的功能，比如：
1. **拦截**：选择性拦截传输的网络流量，比如GFW
2. **统计**：既然所有的流量都经过代理，那么代理也可以用来统计网络中的数据信息，比如了解哪些人在访问哪些网站，通信的应答延迟等
3. **缓存**：如果通信双方比较”远“，访问比较慢，那么代理可以把最近访问的数据缓存在本地，后面的访问不用访问后端来做到加速，比如CDN
4. **分发**（负载均衡）：如果某个通信方有多个服务器后端，代理可以根据某些规则来选择如何把流量发送给多个服务器,即负载均衡功能，比如Nginx
5. **跳板**：如果 A、B 双方因为某些原因不能直接访问，而代理可以和双方通信，那么通过代理，双方可以绕过原来的限制进行通信,比如翻墙
6. **注入**：既然代理可以看到流量，那么也可以修改网络流量，可以自动在收到的流量中添加一些数据，比如有些宽带提供商的弹窗广告

**Service Mesh可以看做是传统代理的升级版，用来解决在微服务框架中出现的问题，即可以把Service Mesh看作是分布式的微服务代理。**

> 在传统模式下，代理一般是集中式的单独的服务器，所有的请求都要先通过代理，然后再流入转发到实际的后端。

在 Service Mesh 中，代理变成了**分布式**的，它常驻在应用的身边。
> 最常见的就是 Kubernetes Sidecar 模式，每一个应用的 Pod 中都运行着一个代理，负责流量相关的事情。

![image](http://p99.pstatp.com/large/pgc-image/15401920246263868d1f552)

这样的话，应用所有的流量都被代理接管，那么这个代理就能做到上面提到的所有可能的事情，从而带来无限的想象力。

- 原来的代理都是基于网络流量的，一般都是工作在 IP 或者 TCP 层，很少关心具体的应用逻辑。
- 在 Service Mesh 中，代理知道整个集群的所有应用信息，并且额外添加了:
    - 热更新、
    - 注入服务发现、
    - 降级熔断、
    - 认证授权、
    - 超时重试、
    - 日志监控等功能

**让这些通用的功能不必每个应用都自己实现，放在代理中即可**。Service Mesh 中的代理对微服务中的应用做了定制化的改进！

![image](http://p99.pstatp.com/large/pgc-image/1540192024607866973174f)

应用微服务之后，每个单独的微服务都会有很多副本，而且可能会有多个版本，这么多微服务之间的相互调用和管理非常复杂，但是有了 Service Mesh，我们可以把这块内容统一在代理层。

如何对这些分布式代理进行统一的管理？手动更新每个代理的配置，对代理进行升级或者维护是不可持续的事情，需要一个控制中心。

管理员只需要根据控制中心的API来配置整个集群的应用流量，安全规则即可，代理会自动和控制中心打交道根据用户的期望改变自己的行为。

![image](http://p99.pstatp.com/large/pgc-image/154019202477706ba677eee)

> 可以理解 Service Mesh 中的代理会抢了 Nginx 的生意，这也是为了 Nginx 也要开始做 NginMesh 的原因。

# Istio
![image](https://preliminary.istio.io/docs/concepts/what-is-istio/arch.svg)

Istio 就是我们上述提到的 Service Mesh 架构的一种实现，服务之间的通信（比如这里的 Service A 访问 Service B）会通过代理（默认是 Envoy）来进行。

而且中间的网络协议支持 HTTP/1.1，HTTP/2，gRPC 或者 TCP，可以说覆盖了主流的通信协议。

控制中心做了进一步的细分，分成了 Pilot、Mixer 和 Citadel，它们的各自功能如下：
- **Pilot**：为 Envoy 提供了服务发现，流量管理和智能路由（AB 测试、金丝雀发布等），以及错误处理（超时、重试、熔断）功能。

用户通过 Pilot 的 API 管理网络相关的资源对象，Pilot 会根据用户的配置和服务的信息把网络流量管理变成 Envoy 能识别的格式分发到各个 Sidecar 代理中。

- **Mixer**：为整个集群执行访问控制（哪些用户可以访问哪些服务）和 Policy 管理（Rate Limit，Quota 等），并且收集代理观察到的服务之间的流量统计数据。
- **Citadel**：为服务之间提供认证和证书管理，可以让服务自动升级成 TLS 协议。

代理会和控制中心通信，一方面可以获取需要的服务之间的信息，另一方面也可以汇报服务调用的 Metrics 数据。

## Istio解决的问题
单个应用拆分成了许多分散的微服务，它们之间相互调用才能完成一个任务，而一旦某个过程出错（组件越多，出错的概率也就越大），就非常难以排查。

用户请求出现问题无外乎两个问题：**错误和响应慢**。

1. 故障排查
2. 应用容错性
3. 应用升级发布
4. 系统安全


## 用什么姿势接入 Istio？

虽然 Istio 能解决那么多的问题，但是引入 Istio 并不是没有代价的。最大的问题是 Istio 的复杂性，强大的功能也意味着 Istio 的概念和组件非常多，要想理解和掌握 Istio ，并成功在生产环境中部署需要非常详细的规划。

一般情况下，集群管理团队需要对 Kubernetes 非常熟悉，了解常用的使用模式，然后采用逐步演进的方式把 Istio 的功能分批掌控下来。

第一步，自然是在测试环境搭建一套 Istio 的集群，理解所有的核心概念和组件。

了解 Istio 提供的接口和资源，知道它们的用处，思考如何应用到自己的场景中，然后是熟悉 Istio 的源代码，跟进社区的 Issues，了解目前还存在的 Issues 和 Bug，思考如何规避或者修复。

这一步是基础，需要积累到 Istio 安装部署、核心概念、功能和缺陷相关的知识，为后面做好准备。

第二步，可以考虑接入 Istio 的观察性功能，包括 Logging、Tracing、Metrics 数据。

应用部署到集群中，选择性地（一般是流量比较小，影响范围不大的应用）为一些应用开启 Istio 自动注入功能，接管应用的流量，并安装 Prometheus 和 Zipkin 等监控组件，收集系统所有的监控数据。

这一步可以试探性地了解 Istio 对应用的性能影响，同时建立服务的性能测试基准，发现服务的性能瓶颈，帮助快速定位应用可能出现的问题。

此时，这些功能可以是对应用开发者透明的，只需要集群管理员感知，这样可以减少可能带来的风险。

第三步，为应用配置 Time Out 超时参数、自动重试、熔断和降级等功能，增加服务的容错性。

这样可以避免某些应用错误进行这些配置导致问题的出现，这一步完成后需要通知所有的应用开发者删除掉在应用代码中对应的处理逻辑。这一步需要开发者和集群管理员同时参与。

第四步，和 Ingress、Helm、应用上架等相关组件和流程对接，使用 Istio 接管应用的升级发布流程。

让开发者可以配置应用灰度发布升级的策略，支持应用的蓝绿发布、金丝雀发布以及 AB 测试。

第五步，接入安全功能。配置应用的 TLS 互信，添加 RBAC 授权，设置应用的流量限制，提升整个集群的安全性。

因为安全的问题配置比较繁琐，而且优先级一般会比功能性相关的特性要低，所以这里放在了最后。

当然这个步骤只是一个参考，每个公司需要根据自己的情况、人力、时间和节奏来调整，找到适合自己的方案。

# 总结

Istio 的架构在数据中心和集群管理中非常常见，每个 Agent 分布在各个节点上（可以是服务器、虚拟机、Pod、容器）负责接收指令并执行，以及汇报信息。

控制中心负责汇聚整个集群的信息，并提供 API 让用户对集群进行管理。

Kubernetes 也是类似的架构，SDN（Software Defined Network） 也是如此。

相信以后会有更多类似架构的出现，这是因为数据中心要管理的节点越来越多，我们需要把任务执行分布到各节点（Agent 负责的功能）。

同时也需要对整个集群进行管理和控制（Control Plane 的功能），完全去中心化的架构是无法满足后面这个要求的。

Istio 的出现为负责的微服务架构减轻了很多的负担，开发者不用关心服务调用的超时、重试、Rate Limit 的实现，服务之间的安全、授权也自动得到了保证。

集群管理员也能够很方便地发布应用（AB 测试和灰度发布），并且能清楚看到整个集群的运行情况。

但是这并不表明有了 Istio 就可以高枕无忧了，Istio 只是把原来分散在应用内部的复杂性统一抽象出来放到了统一的地方，并没有让原来的复杂消失不见。

因此我们需要维护 Istio 整个集群，而 Istio 的架构比较复杂，尤其是它一般还需要架在 Kubernetes 之上，这两个系统都比较复杂，而且它们的稳定性和性能会影响到整个集群。

因此再采用 Isito 之前，必须做好清楚的规划，权衡它带来的好处是否远大于额外维护它的花费，需要有相关的人才对整个网络、Kubernetes 和 Istio 都比较了解才行。