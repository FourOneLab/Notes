# 概述
Spark是一个基于内存的用于处理、分析大数据的集群计算框架。它提供了一套简单的编程接口，从而使得应用程序开发者方便使用集群节点的CPU、内存、存储资源来处理大数据。

## 主要特点
- 使用方便：
Spark提供比MapReduce更简单的编程模型。它提供了80多个用于处理数据的操作符（相比于Hadoop只有2个操作符map & reduce，它要求任何问题都被分解为系列的Map作业和Reduce作业，但是有些算法难以只用map和reduce来描述）。

- 快速：Spark比Hadoop快上若干个数量级，如果数据都加载到内存中，能够快上数百倍，哪怕数据无法全部加载到内存中，Spark也能快上十倍(Spark比Hadoop快的原因：1.使用基于内存计算的集群，2.实现了更先进的执行引擎)。Spark允许应用程序利用内存缓存数据，这样减少磁盘I/O，接下来的所有操作都可以基于缓存的数据进行。

    Spark不会自动将输入数据缓存在内存中，在数据处理的流水线上，何时缓存和缓存哪部分数据完全由应用程序决定。Spark在运行时将一个作业转化为由若干个阶段构成的有向无环图（DAG），并且可以一次执行一个包含多阶段的复杂作业。在作业执行的各个阶段可以进行相应的优化，在Hadoop中一个作业会被划分为由map和reduce组成的有向无环图，如果处理数据的算法很复杂，那么会被划分为多个作业，然后顺序的执行。
    
- 通用：Spark为各种类型的数据处理作业提供一个统一的集成平台，可以用于批处理、交互分析、流处理、机器学习和图计算（Hadoop只适合批处理，对不同类型的数据处理作业，需要使用不同的框架，增加学习成本和维护成本，而且数据在不同的框架中需要有多份，作业执行的代码也必须要重复多份）。

    Spark自带一系列的库，用于批处理、交互分析、流处理、机器学习和图计算。这样可以在单一的框架下创建一个包含多个不同类型任务的数据处理流水线。降低了运维的困难度、减少了代码和数据的重复。

- 可扩容
- 容错



# Spark RDD 怎么分区宽依赖和窄依赖
- 宽依赖：父RDD的分区被子RDD的多个分区使用。例如 groupByKey、reduceByKey、sortByKey等操作会产生宽依赖，会产生shuffle。

- 窄依赖：父RDD的每个分区都只被子RDD的一个分区使用。例如map、filter、union等操作会产生窄依赖

