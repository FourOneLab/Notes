Kubernetes中大部分概念如Node，Pod，Replication Controller，Service等都可以看作是一种==资源对象==。几乎所有的资源对象都是通过kubectl工具进行增删改查等操作，并将数据保存etcd中持久化存储。

所有的资源对象都可以采用**yaml**或**json**格式的文件来定义或描述。

> Kubernetes是一个高度自动化的资源控制系统，它通过跟踪对比etcd库中保存的“资源期望状态”与当前环境中“实际资源状态”的差异来实现自动控制和自动纠错的高级功能。

## Master
Masters是集群的控制节点，负责整个集群的管理和控制，基本上所有的控制命令都发送给它，它负责具体的执行过程。

++高可用部署的时候建议用3台服务器。++

**Master节点上运行的关键进程：**
- **Kubernetes API Server**（==kube-apiserver==）：提供了HTTP Rest接口的关键服务进程，是kubernetes中所有资源的增删改查等操作的唯一入口，也是集群控制的入口进程。
- **Kubernetes Replication Controller**（==kube-controller-manager==）：kubernetes中所有资源对象的自动化控制中心，资源对象的大管家。
- **Kubernetes Scheduler** （==kube-scheduler==）：负责资源调度（Pod调度）的进程。
- **etcd**：kubernetes中所有资源对象的数据全部保存在etcd中。

## Node
Node节点是kubernetes集群中的工作负载节点，当某个node宕机，其上的工作负载会被Master（kube-scheduler）自动转移到其他节点上。

**Node节点上运行的关键进程：**
- **kubelet**（==kubelet==）： 负责pod对应的容器的创建、启停等任务，同时与Master节点密切协作，实现集群管理的基本功能
- **kube-proxy**（==kube-proxy==）：实现kubernetes Service的通信和负载均衡。
- **Docker Engine** （==Dokcer==）：Docker 引擎，负责本机容器的常见和管理工作。

> 当Node节点正确的安装、配置和启动了上述关键进程后，可以在运行期间动态的增加到集群中，默认情况下，kubelet会向Master注册自己

一旦Node节点被纳入到集群中，kubelet会定时向Master节点汇报自身的情报（例如：操作系统，Docker版本，机器的CPU和内存等情况，以及当前有哪些pod在运行），这样Master可以知道每个Node的资源使用情况，实现高效均衡的资源调度。

**当Node超时未上报信息时，会被Master认为失联了，标记为Not Ready，随后Master会触发“工作负载大转移”的自动流程**

## Pod
Pod是Kubernetes最为重要的最基本的概念。

每个pod都有一个特殊的称为根容器的Pause容器（Pause容器对应的镜像属于kubernetes平台的一部分），除了根容器，每个pod还会有若干个业务容器。


#### 设计pod的原因
1. 在一组容器作为一个单元的情况下， 难以对整体简单的进行判断及有效地进行行动。引入业务无关切且不易死亡的Pause容器作为pod的根容器，以它的状态代表整个容器组的状态。
2. pod里的若干个业务容器共享Pause容器的IP，共享Pause容器挂载的Volume，这样简化了业务容器之间的通信，解决了他们之间的文件共享问题。

kubernetes为每个pod分配一个唯一的PodIP，一个pod中的多个容器共享PodIP。
>kubernetes 要求底层网络支持集群内任意两个pod之间的TCP/IP直接通信（通过虚拟二层网络实现，如Flannel、Open vSwatch等）。

因此，kubernetes中一个pod能够与另外一台主机上的其他pod直接通信。

#### pod分类
- 普通pod：一旦创建就会存储到etcd中，随后被Master 的kube-scheduler调度到某个node上进行绑定（binding），随后该pod被对应node上的kubelet实例化成一组相关docker容器并启动起来。
- 静态pod：不存放在etcd中，而是存放在某个具体的Node的某个具体的文件中，并且只能在该Node上启动运行。


默认情况下，当pod中的容器停止时，kubernetes会自动检查到这个问题并重启整个pod中的所有容器。

#### Endpoint
每个Pod都会被分配一个唯一的PodIP，同时在创建pod中的容器时，每个容器都可以设置一个containerPort。

- **Endpoint**=PodIP+ContainerPort，它代表着pod中的某个服务进程的对外通信地址。

#### Pod Volume
定义在Pod之上，被各个容器挂载到自己的文件系统中。

#### Event
事件的记录，记录了事件的：
1. 最早产生时间、
2. 最后重现时间、
3. 重复次数、
4. 发起者、
5. 类型、
6. 导致此事件的原因等

**Event通常会关联到某个具体的对象上**，当某个Pod迟迟未创建时，可以用

```
kubectl describe pod XXX

Events:
  FirstSeen	LastSeen	Count	From			SubObjectPath	Type		Reason		Message
  ---------	--------	-----	----			-------------	--------	------		-------
  17h		3m		208	{kubelet 172.16.140.37}			Normal		Pulling		pulling image "172.16.140.38:5000/transwarp/license:tdc-1.0.0-final"
  17h		3m		208	{kubelet 172.16.140.37}			Normal		Pulled		Successfully pulled image "172.16.140.38:5000/transwarp/license:tdc-1.0.0-final"
  17h		3m		199	{kubelet 172.16.140.37}			Normal		Created		(events with common reason combined)
  17h		3m		199	{kubelet 172.16.140.37}			Normal		Started		(events with common reason combined)
  17h		4s		4396	{kubelet 172.16.140.37}			Warning		BackOff		Back-off restarting failed docker container
  17h		4s		4370	{kubelet 172.16.140.37}			Warning		FailedSync	Error syncing pod, skipping: failed to "StartContainer" for "zookeeper" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=zookeeper pod=zookeepersbf92-2_tdcsys(1341aec1-a469-11e8-8852-0cc47aa5d7ec)"

```

每个pod都可以对其能使用的服务器上的计算资源设置配额(当前可以设置CPU和内存)，其中CPU的资源量为core，内存的资源量为字节数，都是==绝对值而非相对值==。

>一个cpu的配额对于绝大多数容器来说是相当大的一个资源配额，因此以千分之一的CPU配额为最小单位，用m来表示。通常一个容器的cpu配额设置在100 ~ 300m即0.1 ~ 0.3个cpu。

在Kubernetes中，一个计算资源进行配额限定需要设置两个参数：
- Requests：该资源的最小申请量，系统必须满足要求
- Limits：该资源最大允许使用的量，当容器试图使用超过这个量的资源时，可能会到这被kill并重启

**通常Requests会设置为一个比较小的值，满足容器日常的工作负载，Limits设置为峰值工作负载下的资源使用量**


```
spec
 containers:
 - name： db
   images: mysql
   resources:
    requests：
      memory: "64Mi"
      CPU: "250m"
    Limits: 
      memory: "128Mi"
      cpu: "500m"
```

## label
key=value的一个键值对，其中key与value都是用户自定义的，
- label可以附加到各种资源对象上，如Node，Pod，Service，RC等
- 一个资源对象可以定义任意数量的label
- 同一个label可以被添加到任意数量的资源对象上

**label通常在资源对象定义时确定，也可以在对象创建后动态增删。**

通过给一个资源对象捆绑一个或多个不同的label来实现多维度的资源分组管理功能。

常用的label如下：
- 版本标签："release":"stable";"release":"cannary"...
- 环境标签："environment":"dev";"environment":"qa";"environment":"production"
- 架构标签："tier":"frontend";"tier":"backend"；"tier":"middleware"
- 分区标签："partition":"cunstomerA";"partition":"constumerB"
- 质量管理标签："track":"daily";"track":"weekly"

#### Label Selector
标签选择器查询和筛选拥有某些Label的资源对象

- 基于等式的标签选择器（Equality-based）

```
name=redis-slave //匹配所有具有标签name=redis-slave的资源对象

env!= production //匹配所有不具有标签env=production的资源对象
```

- 基于集合的标签选择器（Set-based）

```
name in （redis-master，redis-slave）//匹配所有具有标签name=redis-master或者name=redis-slave的资源对象

name notin（php-frontend）//匹配所有不具有标签name=php-fronted的资源对象
```

通过多个标签选择器的组合实现复杂的条件选择，表达式直接使用“，”逗号分隔，条件之间是"AND"的关系


```
name=redis-slave，env！=production
name notin （php-frontend），env！=production
```

label 定义在metadata中，举例：

```
apiVersion：v1
kind: Pod
metadata:
 name: myweb
 labels:
  app:myweb
```
1. 管理对象RC和Service，在spec中定义Selector与Pod的关联

```
apiVersion：v1
kind： ReplicationController
metadata：
 name：myweb
spec：
 replicas:1
 selector：
  app：myweb
```

```
apiVersion：v1
kind： Service
metadata：
 name：myweb
spec：
 selector：
  app：myweb
 ports：
 - port： 8080
```
2. 新出现的资源对象Deployment，ReplcaSet，DaemonSet和Job可以在Selector中使用基于集合的筛选条件

```
selector：
 matchLabels：
  app：myweb
 matchExpressions:
  - {key:tier,operator: In, values:[frontend]}
  - {key:environment, operator: NotIn, values:[dev]}
```

- matchlabel: 用于定义一组label
- matchExpression：用于定义一组基于集合的筛选条件，可用的条件运算符：In、NotIn、Exists、DoesNotExist

如果同时设置了matchlabel和matchExpression，那么这些条件直接为AND的关系，必须同时满足才能完成selector的筛选。


#### Label Selector的使用场景：
1. kube-controller进程通过对资源对象RC上定义的label selector 来筛选要监控的Pod副本的数量，从而实现Pod副本的数量始终符合预期设定的全自动控制流程
2. kube-proxy进程通过Service的label Selector来选择对用的Pod，自动建立起每个Serivice到对应Pod的请求转发路由表，从而实现Service的智能负载均衡机制
3. 通过对某些node定义特定的label，并且在pod定义文件中使用NodeSelector这种标签调度策略，kube-scheduler进程可以实现Pod定向调度的特性。

**使用Label可以给对象创建多组标签，Label和Label Selector共同构成了Kubernetes系统最核心的应用模型，使得被管理对象能够被精细地分组管理，同时实现了整个集群的高可用性。**


## Replication Controller
RC定义了一个期望的场景，声明某种Pod的副本数量在任意的时刻都符合期望值。

RC的定义包括以下几部分:
1. Pod期望的副本数（replicas）
2. 用于筛选目标Pod的 Label Selector
3. 当Pod的副本数量小于期望数量时，用于创建新Pod的Pod模板

定义好的一个RC提交到Kubernetes集群后，Master节点上的Controller Manager组件就得到通知，定期巡检系统中当前存活的Pod，并确保目标Pod实例数量刚好等于此RC的期望值。

**通过RC实现了用户应用集群的高可用性。** 减少了传统的手工IT运维中工作（主机监控脚本，应用监控脚本，故障恢复脚本）

在运行中可以通过命令，修改RC的副本数量，实现Pod的动态缩放（Scaling）

```
kubectl scale rc pod-name --replicas=N
//pod-name为需要修改副本的pod名字
//N为修改后期望的副本数量
```

删除RC并不会影响已经通过RC创建好的Pod，如果需要删除所有的Pod，可以通过将replicas的值设置为0，然后更新RC。

kubectl也提供了stop和delete命令，一次性删除RC和它控制的全部Pod。

>RC的升级版 Replica Set，两者的唯一区别，Replica Set支持基于集合的Label Selector，RC只支持基于等式的Label Selector。

>当前很少单独使用RS，RS被Deployment这个更高层次的资源对象所使用，从而形成一整套Pod创建、删除、更新的编排机制。

RC（升级版RS）的功能总结：
1. 大多数情况下通过定义一个RC实现Pod的创建过程及副本数量的自动控制
2. RC的定义中包含完整的Pod定义
3. RC通过Label Selector实现对pod副本的自动控制
4. 通过RC中Pod的副本数量，实现Pod 的扩容和缩容
5. 通过改变RC里面Pod的镜像版本来实现Pod的滚动升级

## Deployment 
引入Deployment更好的解决Pod编排的问题，在Deployment的内部使用Replica Set来实现，可以看作是RC的一次升级。

**Deployment相对于RC的一个最大的升级是我们可以随时知道当前Pod部署的进度。**

#### Deployment的典型使用场景
1. 创建一个Deployment对象来生成对应的Replica Set 并完成Pod副本的创建过程
2. 检查Deployment的状态来看部署动作是否完成（Pod副本的数量是否达到预期的值）
3. 更新Deployment以创建新的Pod（镜像升级）
4. 如果当前Deployment不稳定，则回滚到一个早先的Deployment版本
5. 暂停Deployment以便于一次性修改多个PodTemplateSpec的配置项，之后在恢复Deployment，进行新的发布
6. 扩展Deployment以应对高负载
7. 查看Deployment状态，以此作为发布是否成功的指标
8. 清理不再需要的旧版本Replica Set

Deployment的定义与Replica Set 的定义很类似：

```
apiVersion: extensions/v1beta1          apiVersion: v1
kind: Deployment                        kind: ReplicaSet
metadata:                               metadata:
 name:nginx-deployment                   name:nginx-repset
```


```
[root@tdc-01 ~]# kubectl get deployment --all-namespaces
NAMESPACE     NAME                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
96corwk       cas-msvv43m             1         1         1            1           17d
96corwk       cas-srvp9w1n            1         1         1            1           17d

```
上面的输出中涉及的数量解释：
- DESIRED：Pod副本数的期望值，即Deployment里定义的Replicas
- CURRENT：当前Replica的值
- UP-TO-DATE：最新版本的Pod的副本数量，用于指示滚动升级的过程中，有多少个Pod已经升级成功
- AVAILABLE：当前集群中可用的Pod副本数量
- AGE：pod 的存活时间


```
[root@tdc-01 ~]# kubectl get rs --all-namespaces
NAMESPACE     NAME                               DESIRED   CURRENT   READY     AGE
96corwk       cas-msvv43m-4239888530             1         1         1         17d
96corwk       cas-srvp9w1n-3964009551            1         1         1         17d
```


```
[root@tdc-01 ~]# kubectl get po --all-namespaces
NAMESPACE     NAME                                     READY     STATUS             RESTARTS   AGE
96corwk       cas-msvv43m-4239888530-pmwfx             1/1       Running            0          12d
96corwk       cas-srvp9w1n-3964009551-wpq05            1/1       Running            1          12d

```

上面的三个命令对比发现，Pod的命名以Deployment对应的Replica Set的名字为前缀，这种命名很清晰的表明了一个Replica Set创建了哪些Pod，对于Pod的滚动升级，很容易排查错误。

## Horizontal Pod Autoscaler
通过手工执行kubectl scale可以实现Pod扩容或缩容。但是Kubernetes的定位是自动化、智能化，**分布式的系统需要能够++根据当前负载的变化情况++自动触发水平扩展或缩容的行为**。

kubernetes版本 | apiVersion | 描述
---|---|---|
v1.0之前|v1|远古版本
v1.0| extensions/v1bate1|旧版本，HPA为测试版
v1.2| autoscaling/v1|HPA升级为稳定版
v1.6| autoscaling/v2alpha1| HPA功能增加

HPA也属于是资源对象，它的实现原理：++通过追踪分析RC控制的所有目标Pod的负载变化情况，来确实是否需要针对性地调整目标Pod的副本数。++

当前HPA有两种方式作为Pod负载的度量指标：
- CPUUtilizationPercentage（目标Pod所有副本自身CPU利用率的算术平均值）【如果目标Pod没有定义Request值，那么无法计算CPUUtilizationPercentage，来实现横向的自动扩容】
- 应用程序自定义的度量指标（比如，服务在每秒内的相应的请求数TPS/QPS）【实验特性，不能在生产中使用】


CPU利用率=Pod的CPU使用量/Pod的 Request值

CPUUtilizationPercentage的计算过程使用的是Pod的CPU使用量通常是1min内的平均值，目前通过查询Heapster扩展组件来得到这个值（需要安装部署Heapster，这样增加了复杂度，未来Kubernetes自身实现一个基础性能数据采集模块）。

## StatefulSet
在kubernetes中，Pod的管理对象RC、Deployment、DaemonSet和Job都是面向无状态的服务，但是现实中很多服务时有状态的，特别是复杂的中间件集群，如Mysql集群、MongoDB集群、Akka集群、zookeeper集群，这些应用集群有一些共同点：
1. 每个节点都要固定的身份ID，通过这个ID集群中的成员可以相互发现并且通信
2. 集群的规模比较固定，不能随意变动
3. 集群中的每个节点都是有状态的，通常会持久化数据到永久存储中
4. 如果磁盘损坏，则集群里的某个节点无法正常运行，集群功能受损

>如果使用RC或者Deployment控制Pod副本数量的方式来实现上述有状态集群，则无法满足第一个条件，因为Pod的名字是随机产生的，Pod的IP的地址也是在运行期间才确定，切可能有变动，事前无法为每个Pod确定唯一不变的ID。

>为了能够在其他节点上恢复某个失败的节点，这种集群中的Pod需要挂接某种共享存储，为了解决这个问题，v1.4引入了新的资源对象PetSet，在v1.5更名为StatefulSet

**从本质上看，Statefulset是RC或Deployment的变种。**

Stateful Set的特性：
1. Stateful Set中定义的Pod都有稳定的、唯一的网络标识，可以用来发现集群内的其他成员。（假设Stateful set的名字是kafka，那么第一个pod是kafka-0，以此类推）
2. Stateful Set控制的Pod副本的启停顺序是受控制的（要操作第n个Pod，那么前n-1个pod都已经是运行期准备好的状态）
3. Stateful Set里的Pod采用稳定的持久化存储卷，通过PV/PVC来实现，删除pod时默认不会删除与stateful set相关的存储卷（为了保证数据安全）

Stateful Set除了要与PV卷捆绑使用以存储Pod的状态数据，还要与Headless Service配合使用，即在每个Stateful Set的定义中要声明它属于哪个Headless Service。==Headless Service与普通Service的区别在于，它没有Cluster IP，如果解析Headless Service的DNS域名，则返回的是该Service对应的全部Pod的Endpoint列表==。

Stateful Set 在Headless Service 的基础上为每个Pod实例创建一个**DNS域名**，格式如下
```
：$(podname).$(headless service name)
```
还是以上面的kafka stateful set 为例，假设对应的Headless Service的名字是kafka，那么stateful set中3个pod的DNS名称分布为kafka-0.kafka，以此类推，这些DNS的名称可以直接在集群的配置文件中固定下来。

## Service
Service也是Kubernetes的资源对象，每个Service就是经常提到的微服务架构中的一个“微服务”。

![image](https://note.youdao.com/yws/public/resource/2a9d776b887651686e00ee8b72f722ff/xmlnote/F12939BBC16D4ECDBC81F40735E2F7C3/10464)

从图中可以看出，Service定义了一个对外的访问入口地址，前端的应用（Pod）通过这个入口地址访问其背后的一组由Pod副本组成的集群实例，Service与其后端Pod副本集群之间则是通过Label Selector 来实现“无缝对接”，RC的作用是保证Service的服务能力和服务质量始终处于预期的标准。

通过分析、识别并建模系统中所有服务为微服务（kubernetes Service），最终我们系统由多个提供不同业务能力而又彼此独立的微服务单元组成，==服务之间通过TCP/IP进行通信==，形成强大灵活的弹性网格，拥有强大的分布式能力、弹性扩展能力、容错能力。

> 每个Pod都会被分配一个单独的IP地址，而且每个Pod都提供了独立的EndPoint（PodIP+ContainerPort）以被客户端访问，那么多个Pod副本组成的集群来提供服务时，客户端是如何访问他们呢？

一般的做法是部署一个负载均衡器（软件或硬件），为这组pod开启一个对外的服务端口8000，并且将这些pod的EndPoint列表加入8000端口的转发列表中，客户端可以通过负载均衡器的对外IP地址+服务端口来访问此服务，而客户端的请求最后会被转发到哪个pod，由负载均衡器的算法决定。

- Kubernetes也遵循上述做法，运行在每个Node上的kube-proxy进程其实就是一个智能的负载均衡器，它负责把对Service的请求转发到后端某个Pod上，并在内部实现服务的负载均衡与会话保持机制。

- kubernetes中的每个Service并没有共用一个负载均衡器的IP地址，而是每个Servie分配一个全局唯一的虚拟IP（Cluster IP），这样每个Service就变成了具有唯一IP地址的“通信节点”，服务调用变为了最基本的TCP网络通信问题。
- pod 的Endpoint会苏子和pod的销毁重建而改变，但Service创建后的Cluster IP是不会改变的。解决服务发现问题，只需要将Servie的name和Cluster IP 做一个DNS域名映射。

在定义Service时，需要制定暴露出来的端口即targetPort，如果没有定义，则默认targetPort=Port。

#### Service的多端口问题
很多服务都会存在多个端口，一个提供服务，一个提供管理，Kubernetes支持多个Endpoint，只需要在定义时为每个Endpoint指定好名字加以区分。

```
apiVersion：v1
kind：Service
metadata：
 name：tomcat-service
spec：
 ports：
 - port：8080
  name：service-port
 - port：8005
  name：shutdown-port
```

#### 服务发现机制
任何分布式系统都会涉及服务发现这个问题，Kubernetes中的每个Service都有唯一的Cluster IP及唯一的名字，名字在创建时自己指定，可以固定在配置中。

如何通过service的名字找到对应的Cluster IP？
1. 通过Linux环境变量的方式，每个Service生成对应的Linux环境变量，在pod的容器启动的时候，自动注入这些环境变量。所有变量根据统一的命名规范，通过代码访问系统变量就可以实现服务发现。
2. 将服务名作为DNS中的域名，这样程序就可以直接使用服务名来建立通信连接了。


#### 外部系统访问Service问题
在Kubernetes中有三种IP：
- Node IP：Node节点的IP地址
- Pod IP：Pod 的IP地址
- Cluster IP： Service 的IP地址

**Node IP**：集群中每个节点物理网卡的IP地址，所有处于这个网络的服务器都能通过这个网络直接通信，无论他们之间是否有部分节点不属于Kubernetes集群，即kubernetes集群之外的节点访问Kubernetes集群之内某个节点或者TCP/IP服务时，必须通过Node IP进行通信。

**Pod IP**：每个Pod的IP地址,Docker Engine根据docker0网桥的IP地址段进行分配的，通常是一个虚拟的二层网络。当同一个集群中的不同节点间的Pod要进行通信时，通过Pod IP所在的虚拟二层网络进行通信，真实的TCP/IP流量则是通过Node IP所在的网卡流出。

**Cluster IP**：service的虚拟IP地址。具体特点如下：
- 仅作用于Service，通过Kubernetes的Cluster IP地址池管理和分配。
- Cluster IP无法被ping，没有实体网络对象来响应它。
- Cluster IP需要结合Service Port组成一个具体的通信端口，单独的Cluster IP不具备TCP/IP通信能力，它是kubernetes集群内部的，集群之外要访问需要额外的配置。
- Kubernetes集群中Node IP网，Pod IP网，Cluster IP网之间的通信是一种特殊的路由规则，与IP路由不同。

外部系统访问集群中的服务时，最直接有效常用的方法，通过指定Service 的NodePort，例如通过浏览器输入：

```
http：//<nodePort IP>:NodePort/
```
NodePort的实现方式是在Kubernetes集群里的每个Node上为需要外部访问的Service开启一个对应的TCP监听端口，外部系统只要任意一个**Node的IP地址+**具体的**NodePort端口号**即可访问此服务。


```
netstat -tlp | grep nodePort

tcp6  0  0 [::]:nodePort    [::]:*    LISTEN    1125/kube-proxy
```
> 但是NodePort并没有完全解决外部访问Service的所有问题，比如负载均衡问题。在集群中部署一个负载均衡器，外部的请求只需要访问负载均衡器的IP地址，由负载均衡器负责转发流量到后面某个Node的NodePort上。


## Volume（存储卷）
Volume是Pod中能够被多个容器访问的共享目录。
1. 被定义在Pod上，被Pod中的多个容器挂载到具体的文件目录下
2. Volume与Pod 的生命周期相同，但与容器的生命周期不相关，容器终止或重启，volume中的数据不会丢失
3. 支持多种类型的volume（如GlusterFS、Ceph等）

#### 存储卷使用方式：
1. 在Pod上声明一个Volume
2. 在容器里引用该Volume并mount到容器的某个目录下



```
template：
 matadata：
  labels：
   app：app-demo
   tier:frontend
 spec:
  volumes:
   - name: datavol
    emptyDir:{}
  containers:
  - name: tomcat-demo
   image: tomcat
   volumeMounts:
    - mountPath: /mydata-data
      name: datavol
   imagePullPolicyL IfNotPresent
```

除了让一个Pod中的多个容器共享文件，让容器的数据写到宿主机的磁盘上或者写文件到网络存储中，volume还有一个功能：++容器配置文件集中化定义与管理，通过新的资源对象ConfigMap 来实现。++

#### Volume的类型：
1. **emptyDir**：
在Pod分配到Node时创建的，初始内容为空，无需指定宿主机上对应的目录文件，kubernetes会自动分配一个目录，当pod从node上移除时emptyDir中数据也会被永久删除，主要用途：
    - 临时空间，程序运行时的临时目录
    - 长时间任务的中间过程CheckPoin的临时保存目录
    - 多容器的共享目录

用户无法控制emptyDir的存储介质，如果kubelet配置的使用硬盘，那emptyDir也创建在该硬盘上。
2. **hostPath**：为在Pod上挂在宿主机上的文件或目录，主要用途
    - 容器应用程序生成的日志
    - 访问宿主机Docker 引擎内部数据结构的容器应用时，通过定义hostPath为/var/lib/docker目录，使容器内部应用可以直接访问docker内部文件系统
        - 使用以上设置时，需要注意的是不同的Node上具有相同配置的Pod可能会英文宿主机上的目录和文件不同而导致对volume上目录和文件的访问结果不一致
        - 如果使用了资源配额管理，kubernetes无法将hostPath在宿主机上使用的资源纳入管理

```
volumes:
 - name: "persistent-storage"
  hostPath:
   path: "/data"
```

3. **gcePersistentDisk**：谷歌公有云提供的永久磁盘

4. **awsElasticBlockStore**：亚马逊公有云上的永久磁盘

5. **NFS**：使用NFS网络文件系统提供的共享目录存储数据时，需要在系统中部署宇哥NFS Server

```
volumes:
 - name: nfs
  nfs:
   server: nfs-server/localhost //NFS服务器地址
   path: "/data"
```

6. **其他**：iscsi、flocker、glusterfs、rbd、gitRepo、secret（用于为pod提供加密信息，将定义的secret直接挂载为文件当pod访问，通过tmfs实现，所以secret volume总是不会被持久化）


## Persistent Volume
上面的Volume是定义在Pod上的，属于计算资源，而实际上，网络存储是相对独立于计算资源而存在的一种实体资源。

PV是kubernetes集群中某个网络存储中对应的一块存储：
- PV只能属于网络存储，不属于任何Node，但可以在每个Node上访问
- PV独立于Pod之外定义
- PV支持的类型：比volume还要多


```
apiVersion：v1
kind：PersistentVolume
metadata：
 name：pv001
spec：
 capacity：
  storage：5G1
 accessModes：
  - ReadWriteOnce
 nfs：
  path：/data
  server: 172.17.0.2
```

其中PV的accessMode属性的类型：
- ReadWriteOnce：读写权限，单Node挂载
- ReadOnlyMany：只读权限，多Node挂载
- ReadWriteMany：读写权限，多node挂载

**当pod想要申请PV的时候**:
1. 先定义一个PVC：

```
apiVersion:v1
kind:PersistentVolumeClaim
metadata:
 name:myPVC
spec:
 accessModes:
  - ReadWriteOnce
 resources:
  requests:
   storage: 8Gi
```
2. 在pod的Volume定义中引用上述PVC

```
volumes：
 - name：mypd
  persistentVolumeClaim：
   claimName：myPVC
```

#### PV是有状态的
1. Available：空闲状态
2. Bound：已经绑定到某个PVC
3. Released：对应的PVC已经删除，但资源还没有被集群回收
4. Failed：PV自动回收失败


## Namespace
实现多租户的资源隔离，形成逻辑上各组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源的同时还能被管理。

默认创建一个default命名空间，如果不特别指定Namespace，用户创建的Pod、RC、Service都会被系统创建在default下。


```
apiVersion：v1
kind：Namespace
metadata：
 name：newns
```

```
apiVersion：v1
kind：pod
metadata：
 name:busybox
 namespace:newns
spec:
 containers:
 - image:busybox
  command:
   - sleep
   - “3600”
  name：busybox 
```
给每个租户创建Namespace实现多租户隔离的时候，可以结合资源配额管理，限定不同租户能占用的CPU、内存等资源的用量。

## Annotation（注释）
使用key/value键值对的形式定义，通过用Annotation记录如下信息：
- build信息、release信息、Docker镜像信息等
- 日志库、监控库、分析库等资源库的地址信息
- 程序调试工具信息
- 团队信息